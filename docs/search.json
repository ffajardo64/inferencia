[
  {
    "objectID": "chap01.html",
    "href": "chap01.html",
    "title": "1  Conceitos preliminares",
    "section": "",
    "text": "Definição 1.1 (População alvo) Define-se população alvo, ou simplesmente população, a um conjunto de unidades experimentais.\n\n\nDefinição 1.2 (Amostra aleatória) A sequência \\(X_1,X_2,\\ldots,X_n\\) de variáveis aleatórias independentes e identicamente distribuídas define uma amostra aleatória de tamanho \\(n\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSe as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) configuram uma amostra aleatória de uma população com distribuição contínua, então a função de densidade conjunta é dada por \\[f(x_1,x_2,\\ldots,x_n)=f(x_1)f(x_2)\\cdots f(x_n)=\\prod_{i=1}^nf(x_i), \\tag{1.1}\\] onde \\(f(\\cdot)\\) representa a função de densidade marginal para cada variável aleatória \\(X_i\\), com \\(i=1,2,\\ldots,n\\). Nesse caso a Equação 1.1 representa a distribuição da amostra aleatória.\n\n\n\nExemplo 1.1 Seja \\(X\\) uma variável aleatória de uma população com densidade \\(f(\\cdot)\\). Seja \\(X_1,X_2\\) uma amostra aleatória de tamanho \\(2\\). Por definição, a distribuição conjunta de \\(X_1,X_2\\) é dada por \\[f(x_1,x_2)=f(x_1)f(x_2).\\]\n\n\nExemplo 1.2 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\), i.e.,\n\\[\\mathbb{P}[X=x]=p^x(1-p)^{1-x}\\mathbb{I}_{\\{0,1\\}}(x).\\] Daí,\n\n\\[\\begin{align*}\n    \\mathbb{P}[X_1=x_1,X_2=x_2]&=\\mathbb{P}[X_1=x_1]\\mathbb{P}[X_2=x_2]\\\\\n    &=p^{x_1}(1-p)^{1-x_1}\\mathbb{I}_{\\{0,1\\}}(x_1)p^{x_2}(1-p)^{1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_2)\\\\\n    &=p^{x_1+x_2}(1-p)^{2-x_1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_1)\\mathbb{I}_{\\{0,1\\}}(x_2).\n  \\end{align*}\\]\n\nNesse caso, a distribuição conjunta da amostra está definida no pares \\((x_1,x_2)\\in\\{(0,0), (0,1), (1,0), (1,1)\\}\\).\nCabe ressaltar que, a distribuição conjunta da amostra não é igual à distribuição obtida considerando os elementos de uma distribuição tipo binomial. No caso da distribuição binomial, seja \\(Y=X_1+X_2\\) (com prob. \\(1\\)) então \\[\\mathbb{P}[Y=y]=\\binom{2}{y}p^y(1-p)^{2-y}\\mathbb{I}_{\\{0,1,2\\}}(y).\\] Dessa forma, a distribuição binomial é definida para uma única variável que representa o número de sucessos, e difere da distribuição conjunta da amostra porque a binomial não considera a ordem da coleta, por exemplo, para a distribuição conjunta da amostra, \\(\\mathbb{P}[X_1=0,X_2=1]\\) representa a probabilidade de coletar primeiro \\(0\\) e depois \\(1\\)."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Introdução",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. \"Literate Programming\". Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "chap02.html#sec-momentos_amostrais",
    "href": "chap02.html#sec-momentos_amostrais",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.1 Momentos amostrais",
    "text": "2.1 Momentos amostrais\n\nDefinição 2.2 (Momento amostral em torno de zero) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno de zero, define-se como \\[M_r'=\\frac1n\\sum_{i=1}^nX_i^r.\\]\n\n\nDefinição 2.3 (Momento amostral em torno da média amostral) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno da média amostral, define-se como \\[M_r=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^r.\\]\n\n\nExemplo 2.2 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a Definição 2.1, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\n\n\n\n\n\nLembre que…\n\n\n\n\n\nSe \\(X\\) é uma variável aleatória, o \\(r\\)-ésimo momento de \\(X\\) define-se como \\(\\mathbb{E}[X^r]=\\mu_r'\\), se existe. Também pode-se definir o \\(r\\)-ésimo momento central como \\(\\mathbb{E}[(X-\\mu_X)^r]=\\mu_r\\), onde \\(\\mu_X=\\mathbb{E}[X]=\\mu_1'\\).\nPara \\(r=2\\), temos que \\(\\mathbb{E}[X-\\mu_X]^2=\\mathrm{Var}[X]\\).\n\n\n\n\nExemplo 2.3 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.4 Seja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r>-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 2.5 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 2.1, sabemos da Definição 2.2 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 2.4 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n>1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 2.6 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n>1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n>1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 2.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 2.4. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n>1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^k\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n>1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i<k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]"
  },
  {
    "objectID": "chap03.html#sec-prop_estimadores",
    "href": "chap03.html#sec-prop_estimadores",
    "title": "3  Estimação pontual",
    "section": "3.1 Propriedades dos estimadores",
    "text": "3.1 Propriedades dos estimadores\n\nDefinição 3.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Billingsley, Patrick. 1986. Probability and Measure. Second.\nJohn Wiley; Sons.\n\n\nCurtiss, J. H. 1942. “A Note on the Theory Moment Generating\nFunctions.” The Annals of the Mathematical Statistics 13\n(4): 430–33.\n\n\nDudewicz, E., and S. Mishra. 1988. Modern Mathematical\nStatistics. 1st ed. John Wiley & Sons.\n\n\nFeller, W. 1971. An Introduction to Probability Theory and Its\nApplications. 2nd ed. Vol. II. Wiley.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nMood, A., F. Graybill, and D. Boes. 1974. Introduction to the Theory\nof Statistics. 3rd ed. McGraw-Hill Higher Education.\n\n\nShohat, J. A., and J. D. Tamarkin. 1970. The Problem of\nMoments. Vol. I. American Mathematical Society."
  },
  {
    "objectID": "matematica.html",
    "href": "matematica.html",
    "title": "Resultados matemáticos",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. \"Literate Programming\". Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introdução à Inferência Estatística",
    "section": "",
    "text": "Prefácio\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\nMostrar script\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "matematica.html#somas-e-produtos",
    "href": "matematica.html#somas-e-produtos",
    "title": "Resultados matemáticos",
    "section": "Somas e Produtos",
    "text": "Somas e Produtos\nAlgumas fórmulas envolvendo somas são listadas a seguir:\n\n\\(\\sum_{i=1}^ni=\\frac{n(n+1)}{2}\\);\n\\(\\sum_{i=1}^ni^2=\\frac{n(n+1)(2n+1)}{6}\\);\n\\(\\sum_{i=1}^ni^3=\\left[\\frac{n(n+1)}{2}\\right]^2\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^nb_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ib_j\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)^2=\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^na_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ia_j\\);\nTeorema multinomial: \\(\\left(\\sum_{i=1}^na_i\\right)^n=\\sum_{k_1,k_2,\\ldots,k_m}^n\\frac{n!}{k_1!k_2!\\cdots k_m!}x_1^{k_1}x_2^{k_2}\\cdots x_m^{k_m}\\), onde \\(\\sum_{i=1}^mk_i=n\\);\n\nBinomio de Newton: \\((a+b)^n=\\sum_{i=0}^n\\binom{n}{i}a^ib^{n-j}\\);\nTrinomio de Newton: \\((a+b+c)^n=\\sum_{i=0}^n\\sum_{j=0}^n\\frac{n!}{(n-i)!(i-j)!j!}a^{n-i}b^{i-j}c^j\\);"
  },
  {
    "objectID": "matematica.html#fatoriais-e-combinatórias",
    "href": "matematica.html#fatoriais-e-combinatórias",
    "title": "Appendix A — Resultados matemáticos",
    "section": "A.2 Fatoriais e Combinatórias",
    "text": "A.2 Fatoriais e Combinatórias\n\nFunção gamma: \\(\\Gamma(x)=\\int_0^\\infty t^{x-1}e^{-t}\\,dt\\), para \\(x>0\\). Dessa forma, \\(\\Gamma(x+1)=x\\Gamma(x)\\);\n\nSe \\(n\\) é inteiro, então \\(\\Gamma(n+1)=n!\\), onde \\(n!=1\\cdot2\\cdot3\\cdots (n-1)\\cdot n\\);\nSe \\(n\\) é inteiro, \\(\\Gamma\\left(n+\\frac12\\right)=\\frac{1\\cdot3\\cdot5\\cdots(2n-1)}{2^n}\\sqrt\\pi\\). Em particular, \\(\\Gamma\\left(\\frac32\\right)=\\frac12\\Gamma\\left(\\frac12\\right)=\\frac12\\sqrt\\pi\\) ou \\(\\Gamma\\left(\\frac12\\right)=\\sqrt\\pi\\);\nFórmula de Stirling: \\(n!\\approx\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n\\)\n\nFunção beta: \\(B(a,b)=\\int_0^1x^{a-1}(1-x)^{b-1}\\,dx\\), para \\(a>0\\) e \\(b>0\\). De outra forma, \\(B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\);\n\\(\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\);\n\n\\(\\binom{n}{0}=\\binom{n}{n}=1\\);\n\\(\\binom{n}{k}=\\binom{n}{n-k}\\);\n\\(\\binom{n+1}{k}=\\binom{n}{k}+\\binom{n}{k-1}\\), para \\(n=1,2,\\ldots\\) e \\(k=0,\\pm1,\\pm2,\\ldots\\);\n\\(\\binom{-n}{k}=\\frac{(-n)(-n-1)\\cdots(-n-k+1)}{k!}=(-1)^k\\binom{n+k-1}{k}\\);\n\n\n\n\n\n\n\nMood, A., F. Graybill, e D. Boes. 1974. Introduction to the theory of statistics. 3rd ed. McGraw-Hill Higher Education."
  },
  {
    "objectID": "matematica.html#sec-somas",
    "href": "matematica.html#sec-somas",
    "title": "Appendix A — Resultados matemáticos",
    "section": "A.1 Somas e Produtos",
    "text": "A.1 Somas e Produtos\nAlgumas fórmulas envolvendo somas são listadas a seguir:\n\n\\(\\sum_{i=1}^ni=\\frac{n(n+1)}{2}\\);\n\\(\\sum_{i=1}^ni^2=\\frac{n(n+1)(2n+1)}{6}\\);\n\\(\\sum_{i=1}^ni^3=\\left[\\frac{n(n+1)}{2}\\right]^2\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^nb_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ib_j\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)^2=\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^na_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ia_j\\);\nTeorema multinomial: \\(\\left(\\sum_{i=1}^na_i\\right)^n=\\sum_{k_1,k_2,\\ldots,k_m}^n\\frac{n!}{k_1!k_2!\\cdots k_m!}x_1^{k_1}x_2^{k_2}\\cdots x_m^{k_m}\\), onde \\(\\sum_{i=1}^mk_i=n\\);\n\nBinomio de Newton: \\((a+b)^n=\\sum_{i=0}^n\\binom{n}{i}a^ib^{n-j}\\);\nTrinomio de Newton: \\((a+b+c)^n=\\sum_{i=0}^n\\sum_{j=0}^n\\frac{n!}{(n-i)!(i-j)!j!}a^{n-i}b^{i-j}c^j\\);"
  },
  {
    "objectID": "chap02.html",
    "href": "chap02.html",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "",
    "text": "3 Tipos de convergência"
  },
  {
    "objectID": "chap02.html#chap-convergencia",
    "href": "chap02.html#chap-convergencia",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.2 Tipos de convergência",
    "text": "2.2 Tipos de convergência"
  },
  {
    "objectID": "chap04.html#sec-prop_estimadores",
    "href": "chap04.html#sec-prop_estimadores",
    "title": "4  Estimação pontual",
    "section": "4.1 Propriedades dos estimadores",
    "text": "4.1 Propriedades dos estimadores\n\nDefinição 4.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "chap02.html#sec-exercicios_chap02",
    "href": "chap02.html#sec-exercicios_chap02",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.2 Exercícios sugeridos",
    "text": "2.2 Exercícios sugeridos"
  },
  {
    "objectID": "chap02.html#exercícios-sugeridos",
    "href": "chap02.html#exercícios-sugeridos",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "Exercícios sugeridos",
    "text": "Exercícios sugeridos\n\nSeja \\(X\\) uma variável aletória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\). Verifique que \\(\\mathbb{E}[X]=\\mathcal{M}'_X(1)=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}\\) e \\(\\mathrm{Var}[X]=\\mathcal{M}''_X(1)+\\mathcal{M}'_X(1)-\\left[\\mathcal{M}'_X(1)\\right]^2\\).\nSejam \\(X\\) e \\(Y\\) variáveis aleatórias com funções geradoras de probabilidade \\(\\mathcal{M}_X(t)\\) e \\(\\mathcal{M}_Y(t)\\), respectivamente. Mostre que \\(\\mathcal{M}_X(t)=\\mathcal{M}_Y(t)\\) se e somente se \\(\\mathbb{P}[X=k]=\\mathbb{P}[Y=k]\\).\nVerifique que \\(\\mathbb{E}[X^r]=\\sum_{j=0}^r{n\\brace k}\\mathbb{E}[(X)_j]\\), onde \\({n\\brace k}=\\frac{1}{k!}\\sum_{i=0}^k(-1)^{k-i}\\binom{k}{i}i^n=\\sum_{i=0}^k\\frac{(-1)^{k-i}i^n}{(k-i)!i!}\\) são chamados de números de Stirling tipo II.\nSeja \\(X\\) uma variável aleatória de uma população com distribuição hipergeométrica com parâmetros \\(N, K\\) e \\(n\\), i.e., \\(\\mathbb{P}[X=x]=\\frac{\\dbinom{K}{x}\\dbinom{N-K}{n-x}}{\\dbinom{N}{n}}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Mostre que \\(\\mathbb{E}[(X)_r]=\\frac{\\dbinom{K}{r}\\dbinom{n}{r}}{\\dbinom{N}{r}}r!\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\). Mostre que \\(\\mathbb{E}[X-\\lambda]^3=\\lambda\\) e \\(\\mathbb{E}[X-\\lambda]^4=\\lambda+3\\lambda^3\\).\nSeja \\(X\\) uma variável aleatória com função geradora de momentos \\(m(t)=\\exp\\{4(e^{t}-1)\\}\\). Qual o valor de \\(\\mathbb{P}[\\mu-2\\sigma<X<\\mu+2\\sigma]\\)?\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Irwin-Hall com parâmetro \\(n\\), i.e.\n\n\\[f(x;n)=\\frac{1}{(n-1)!}\\sum_{k=0}^n(-1)^k\\binom{n}{k}(x-k)_+^{n-1}\\mathbb{I}_{[0,n]}(x),\\] onde \\((x-k)_+^{n-1}=\\begin{cases}x-k, & x-k\\ge0;\\\\ 0, & x-k<0.\\end{cases}\\)\nUtilize a FGM para calcular a variância de \\(X\\).\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão e seja \\(Y=e^X\\). Calcule a FGM e o \\(r\\)-ésimo momento da distribuição de \\(Y\\).\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição \\(F\\) com média \\(\\mu\\) e variância \\(\\sigma^2\\). Suponha que \\(\\mathbb{E}[X^r]=\\mu'_r\\) e \\(\\mathbb{E}[(X-\\mu)^r]=\\mu_r\\) existem, para todo \\(r=1,2,\\ldots\\). Mostre que\n\n\\(\\mathbb{E}\\left[\\bar{X}_n^3\\right]=\\frac{1}{n^2}\\left(\\mu'_3+3(n-1)\\mu'_2\\,\\mu+(n-1)(n-2)\\mu^3\\right)\\);\n\\(\\mathbb{E}\\left[\\bar{X}_n^4\\right]=\\frac{1}{n^3}\\left(\\mu'_4+4(n-1)\\mu'_3\\,\\mu+6(n-1)(n-2)\\mu'_2\\,\\mu^2  +3(n-1)\\mu'^{\\,2}_2+(n-1)(n-2)(n-3)\\mu^4\\right)\\).\n\nSeja \\(U\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\([0,1]\\). Utilize a FGM para calcular a média e a variância de \\(Y=\\tan\\left(\\left(U-\\frac12\\right)\\pi\\right)\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão. Mostre que\n\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r=&\n  \\begin{cases}\n  2^{r/2}\\frac{\\Gamma\\left(\\frac{r+1}{2}\\right)}{\\Gamma\\left(\\frac12\\right)}, & r \\text{ par};\\\\\n  0, & r \\text{ ímpar}.\n  \\end{cases}\n\\end{align*}\\]\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição binomial com parâmetros \\(n\\) e \\(p\\). Mostre que\n\n\\(\\mu_{r+1}=p(1-p)\\left[\\frac{\\partial\\mu_r}{\\partial p}+nr\\mu_{r-1}\\right]\\);\n\\(\\mathbb{P}[X=x+1]=\\frac{n-x}{x+1}\\frac{p}{1-p}\\mathbb{P}[X=x]\\), para \\(x=0,1,2,\\ldots,n-1\\).\n\n\n\n\n\n\nBillingsley, Patrick. 1986. Probability and Measure. Second. John Wiley; Sons.\n\n\nCurtiss, J. H. 1942. \"A note on the theory moment generating functions\". The annals of the mathematical statistics 13 (4): 430–33.\n\n\nDudewicz, E., e S. Mishra. 1988. Modern mathematical statistics. 1st ed. John Wiley & Sons.\n\n\nFeller, W. 1971. An introduction to probability theory and its applications. 2nd ed. Vol. II. Wiley.\n\n\nShohat, J. A., e J. D. Tamarkin. 1970. The problem of moments. Vol. I. American Mathematical Society."
  },
  {
    "objectID": "chap02.html#sec-fgm",
    "href": "chap02.html#sec-fgm",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.2 Funções geradoras de momentos",
    "text": "2.2 Funções geradoras de momentos\n\nDefinição 2.5 (Função geradora de momentos) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos (FGM) de \\(F_X\\), define-se como \\[M_X(t)=\\mathbb{E}\\left[e^{tX}\\right],\\] se existe para todo \\(t\\), tal que \\(-h<t<h\\), para algum \\(h>0\\).\n\n\n\nTeorema 2.3 Seja \\(M_X(t)\\) a FGM de uma distribuição \\(F_X\\). Se existe \\(M_X(t)\\) para todo \\(|t|<h\\), para algum \\(h>0\\), então \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existe e\n\\[\\mathbb{E}X^r=M_X^{(r)}(0)=\\frac{\\partial^r}{\\partial t^r}M_X(t)\\Biggr|_{t=0}.\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPela expansão em série de Taylor da função exponencial (vide Apêndice A), temos que \\[e^x=1+x+\\frac{1}{2!}x^2+\\frac{1}{3!}x^3+\\frac{1}{4!}x^4+\\cdots.\\]\nPor outra parte, pela Definição 2.5, temos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), então\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\mathbb{E}\\left[\\sum_{j=0}^\\infty\\frac{1}{j!}(tX)^j\\right]\n  =\\sum_{j=0}^\\infty\\frac{t^j}{j!}\\mathbb{E}\\left[X^j\\right].\n\\end{align*}\\]\n\nPor hipóteses, \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existem para todo \\(|t|<h\\), para algum \\(h>0\\).\nDiferenciando em ambos os lados da igualdade e avaliando em \\(t=0\\), temos que\n\n\\[\\begin{align*}\n  M'_X(0)&=\\left(0+\\mathbb{E}X+\\mathbb{E}X^2t+\\cdots+\\mathbb{E}\\left[X^r\\right]\\frac{t^{r-1}}{(r-1)!}\\right)\\Biggr|_{t=0}=\\mathbb{E}X. \\quad\\checkmark\n\\end{align*}\\]\n\nEm geral,\n\n\\[\\begin{align*}\n  M_X^{(r)}(0)&=\\frac{\\partial^r}{\\partial t}M_x(t)\\Biggr|_{t=0}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial t}\\frac{\\partial^{r-1}}{\\partial t^{r-1}}e^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}\\left[X^re^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}X^r. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.7 Seja \\(X\\) uma variável aleatória de uma população com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\), i.e. \\[f(x; \\alpha, \\beta)=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\mathbb{I}_{(0,\\infty)}(x).\\] Utilize a FGM para calcular a variância de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela definição Definição 2.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), para todo \\(t\\), tal que \\(|t|<h\\), para algum \\(h>0\\). Dessa forma,\n\n\\[\\begin{align*}\n  M_X(t)&=\\int_0^\\infty e^{tx}\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx\n  =\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx.\n\\end{align*}\\]\n\nPara avaliar a existência da integral, consideremos os seguintes casos:\n\nSe \\(t=\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lim_{a\\to\\infty}\\int_0^a x^{\\alpha-1}\\,dx=\\frac{\\beta^\\alpha}{\\alpha\\Gamma(\\alpha)}\\lim_{a\\to\\infty} a^{\\alpha}.\n\\end{align*}\\]\n\nPara \\(\\alpha>0\\), \\(\\lim_{a\\to\\infty} a^{\\alpha}\\) é infinito. Dessa forma, \\(M_X(t)\\) não está definida.\n\nSe \\(t>\\beta\\), a quantidade \\(-(\\beta-t)>0\\), então \\(e^{-(\\beta-t)x}>x^{\\alpha-1}\\). Assim, a integral é divergente e, por conseguinte, a FGM não existe.\nSe \\(t<\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx\n  =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\int_0^\\infty u^{\\alpha-1}e^{-u}\\,du, \\quad u=(\\beta-t)x\\\\\n  &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\Gamma(\\alpha)\n  =\\frac{\\beta^\\alpha}{(\\beta-t)^\\alpha}=\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}. \\quad\\checkmark\n\\end{align*}\\]\n\nPara calcular a variância, utilizamos o resultado do Teorema 2.3. Daí\n\n\\[\\begin{align*}\n  M'_X(0)&=\\frac{\\alpha}{\\beta}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha-1}\\Biggr|_{t=0}=\\frac{\\alpha}{\\beta}\\\\\n  M''_X(0)&=\\frac{\\alpha(\\alpha+1)}{(\\beta-x)^2}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}\\Biggr|_{t=0}\n  =\\frac{\\alpha(\\alpha+1)}{\\beta^2}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathrm{Var}[X]&=\\mathbb{E}X^2-\\left(\\mathbb{E}X\\right)^2=\\frac{\\alpha(\\alpha+1)}{\\beta^2}-\\left(\\frac{\\alpha}{\\beta}\\right)^2=\\frac{\\alpha}{\\beta^2}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.8 Seja \\(X\\) uma variável aleatória de uma população com distribuição geométrica com parâmetro \\(p\\), i.e.. \\(\\mathbb{P}[X=x]=p(1-p)^x\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Apresente uma forma analítica para calcula o \\(r\\)-ésimo momento da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx}p(1-p)^x=p\\sum_{x=0}^\\infty \\left(e^{t}(1-p)\\right)^x=\\frac{p}{1-(1-p)e^t}.\n\\end{align*}\\]\n\nA última soma pode ser tratada como uma série geométrica. Nesse caso, a soma converge quando \\((1-p)e^t<1\\) ou \\(t<-\\log(1-p)\\).\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{p(1-p)e^t}{(1-(1-p)e^t)^2}\\Biggr|_{t=0}=\\frac{1-p}{p};\\\\ \\\\\n  \\mathbb{E}\\left[X^2\\right]&=\\frac{p(1-p)e^t(1+(1-p)e^t)}{(1-(1-p)e^t)^3}\\Biggr|_{t=0}=\\frac{(1-p)(2-p)}{p^2}.\n\\end{align*}\\]\n\nA variância é dada por \\(\\mathrm{Var}[X]=\\mathbb{E}X^2-(\\mathbb{E}X)^2=\\dfrac{(1-p)(2-p)}{p^2}-\\dfrac{(1-p)^2}{p^2}=\\dfrac{1-p}{p^2}\\).\nO cálculo das derivadas de ordem superior tornam trabalhoso o uso dessa estratégia. Uma forma analítica para o \\(r\\)-ésimo momento pode ser obtida como\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X^r\\right]&=\\sum_{x=0}^\\infty x^r(1-q)q^x=\\sum_{x=0}^\\infty x^{r-1}(1-q)qxq^{x-1}=(1-q)q\\sum_{x=0}^\\infty x^{r-1}xq^{x-1}\\\\\n  &=(1-q)q\\sum_{x=0}^\\infty x^{r-1}\\frac{\\partial}{\\partial q}q^{x}=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\sum_{x=0}^\\infty x^{r-1}q^{x}\\right]=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\sum_{x=0}^\\infty x^{r-1}(1-q)q^{x}\\right]\\\\\n  &=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\mathbb{E}X^{r-1}\\right],\n\\end{align*}\\]\n\nonde \\(q=1-p\\) e \\(r=1, 2, \\ldots\\). \\(\\quad\\checkmark\\)\n\n\n\n\nExemplo 2.9 Seja \\(X\\) uma variável aleatória de uma população com distribuição Pareto com parâmetros \\(\\alpha>0\\) e \\(\\beta>0\\), i.e. \\[f(x;\\alpha,\\beta)=\\beta\\frac{\\alpha^\\beta}{x^{\\beta+1}}\\mathbb{I}_{[\\alpha,\\infty)}(x).\\] Utilize a FGM da distribuição de \\(X\\) para calcular a sua variância.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.5, temos que\n\n\\[\\begin{align*}\n  M_X(t)&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty\\frac{e^{tx}}{x^{\\beta+1}}\\,dx.\n\\end{align*}\\]\n\nNote que, quando \\(x\\to\\infty\\), \\(\\frac{e^{tx}}{x^{\\beta+1}}\\to\\infty\\) fazendo com que a FGM não exista.\nNo entanto, pode-se observar que para alguns valores específicos de \\(\\beta\\), alguns momentos da distribuição de \\(X\\) podem existir. Por exemplo, considere-se o caso \\(\\beta>r\\), daí\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty \\frac{x^r}{x^{\\beta+1}}\\,dx\n  =\\beta\\alpha^\\beta\\int_\\alpha^\\infty x^{r-\\beta-1}\\,dx\\\\\n  &=-\\beta\\alpha^\\beta\\frac{x^{-\\beta+r}}{\\beta-r}\\Biggr|_\\alpha^\\infty=\\frac{\\beta\\alpha^r}{\\beta-r}. \\quad\\checkmark\n\\end{align*}\\]\n\nEm particular, se \\(\\beta>2\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X&=\\frac{\\beta\\alpha}{\\beta-1} & &\\mathrm{ e } &  \\mathbb{E}X^2&=\\frac{\\beta\\alpha^2}{\\beta-2}.\n\\end{align*}\\]\n\nA variância de \\(X\\) é dada por\n\\[\\mathrm{Var}[X]=\\frac{\\beta\\alpha^2}{\\beta-2}-\\left(\\frac{\\beta\\alpha}{\\beta-1}\\right)^2=\\frac{\\alpha\\beta^2}{(\\beta-1)^2(\\beta-2)}. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(M_Y(t)=\\mathbb{E}\\left[e^{at}e^{btX}\\right]=e^{at}\\mathbb{E}\\left[e^{btX}\\right]=e^{at}M_X(bt)\\), com \\(|t|<\\frac{h}{|b|}\\), \\(h>0\\).\n\n\n\nExemplo 2.10 Seja \\(X\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\((0,1)\\), i.e. \\(f(x)=\\mathbb{I}_{(0,1)}(x)\\). Seja \\(y=(b-a)X+a\\), onde \\(a,b\\) são constantes, tal que \\(b>a\\). Encontre a FGM da distribuição de \\(Y\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM da distribuição de \\(X\\) é dada por\n\\[M_X(t)=\\mathbb{E}[e^{tX}]=\\int_0^1e^{tx}\\,dx=\\frac{e^t-1}{t}, \\quad t\\ne0.\\] Então, a FGM da distribuição de \\(Y\\) é dada por\n\\[M_Y(t)=e^{at}\\frac{e^{(b-a)t}-1}{(b-a)t}=\\frac{e^{bt}-e^{at}}{(b-a)t}, \\quad t\\ne0. \\quad\\checkmark\\]\n\n\\(Y\\) é uma variável aleatória de uma população com distribuição uniforme no intervalo \\((a,b)\\). O \\(r\\)-ésimo momento da distribuição de \\(Y\\) é dado por \\[\\mathbb{E}Y^r=\\int_a^b\\frac{y^r}{b-a}\\,dy=\\frac{b^{r+1}-a^{r+1}}{(n+1)(b-a)}.\\]\n\n\n\n\n\n\nTeorema 2.4 Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias com FGM \\(M_{X}(t)\\) e \\(M_{Y}(t)\\), respectivamente. Se existe \\(h>0\\), tal que \\(M_{X}(t)=M_{Y}(t)\\), para todo \\(|t|<h\\), então \\(F_X(u)=F_Y(u)\\) para todo \\(u\\in\\mathbb{R}\\).\n\n\n\nA prova do Teorema 2.4 requer alguns conceitos que extrapolam o nivel do conteúdo. Para os detalhes sugere-se, e.g., Billingsley (1986) e Curtiss (1942).\n\n\n\n\n\n\n\nProblema dos momentos\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição \\(F(x)\\). Suponha que todos os momentos de \\(F(x)\\) existem, i.e. \\(\\mathbb{E}X\\), \\(\\mathbb{E}X^2\\), \\(\\mathbb{E}X^3\\ldots\\) existem. A distribuição de \\(X\\) pode ser determinada de forma única pelos seus momentos?\nMuitas vezes refere-se a esse questionamento como Problema dos momentos de Hausdorff, em honor ao matemático alemão Felix Hausdorff (1868-1942). Em \\(1921\\), Hausdorff mostrou as condições necessárias e suficientes para determinar de forma única a distribuição de uma variável aleatória a partir dos seus momentos populacionais em torno de zero. Na literatura, muitos autores discutem amplamente os detalhes desse problema e as propostas para sua solução. Sugere-se a leitura, e.g., Dudewicz e Mishra (1988, pp. 261), Shohat e Tamarkin (1970) e Feller (1971, pp. 224).\n\n\n\n\nTeorema 2.5 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de distribuição \\(F\\). Seja \\(Y=\\sum_{i=1}^na_iX_i\\), com \\(a_1,a_2,\\ldots,a_n\\) constantes. A FGM da distribuição de \\(Y\\) é dada por \\[M_{Y}(t)=\\prod_{i=1}^nM_{X_1}(a_it),\\] para todo \\(|t|<h\\), , para algum \\(h>0\\).\n\n\n\n\n\n\n\n\nProva\n\n\n\n\n\nO resultado é consequência da Definição 2.5 e do fato que as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) são independentes e identicamente distribuídas. Assim,\n\n\\[\\begin{align*}\n  M_Y(t)&=\\mathbb{E}\\left[e^{tY}\\right]=\\mathbb{E}\\left[e^{t\\sum_{i=1}^na_iX_i}\\right]\n  =\\mathbb{E}\\left[\\prod_{i=1}^ne^{a_itX_i}\\right]\n  =\\prod_{i=1}^n\\mathbb{E}\\left[e^{a_itX_i}\\right]=\\prod_{i=1}^nM_{X_1}(a_it).\\quad\\blacksquare\n\\end{align*}\\]\n\n\nSe \\(a_1=a_2=\\cdots=a_n\\), então \\(M_Y(t)=\\left[M_{X_1}(a_it)\\right]^n\\).\n\n\n\n\n\nExemplo 2.11 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Mostre que \\(\\bar{X}_n\\) tem distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando o resultado do Teorema 2.5, temos que\n\n\\[\\begin{align*}\n  M_{\\bar{X}_n}(t)&=\\mathbb{E}\\left[e^{t\\frac1n\\sum_{i=1}^nX_i}\\right]\n  =\\left[M_{X_1}\\left(\\frac{t}{n}\\right)\\right]^n\n  =\\left[\\exp\\left\\{\\frac{\\mu}{n}t+\\frac12\\left(\\frac{\\sigma t}{n}\\right)^2\\right\\}\\right]^n\n  =\\exp\\left\\{\\mu t+\\frac{1}{2n}\\left(\\sigma t\\right)^2\\right\\}, t\\in\\mathbb{R}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.12 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGM da distribuição de \\(Y=\\sum_{i=1}^nX_i\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx} \\frac{e^{-\\lambda}\\lambda^x}{\\lambda!}=e^{-\\lambda}\\sum_{x=0}^\\infty  \\frac{(\\lambda e^t)^x}{\\lambda!}=e^{-\\lambda}e^{\\lambda e^{t}}=e^{\\lambda(e^t-1)}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí, pelo Teorema 2.5, temos que\n\n\\[\\begin{align*}\n  M_Y(t)&=\\left(e^{\\lambda(e^t-1)}\\right)^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\n\nDessa forma, \\(Y\\) é uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(n\\lambda\\).\n\n\n\n\n\n\nTeorema 2.6 Seja \\(X_1,X_2,\\ldots,X_k\\) uma sequência de variáveis aleatórias independentes com distribuição normal com média \\(\\mu_i\\) e variância \\(\\sigma^2_i\\), para \\(i=1,2,\\ldots, k\\). Então \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2,\\] segue uma distribuição \\(\\chi^2_{(k)}\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nSeja \\(Z_i=\\frac{X_i-\\mu_i}{\\sigma_i}\\). Dessa forma, a sequência \\(Z_1,Z_2,\\ldots,Z_n\\) configura uma amostra aleatória de uma população com distribuição normal padrão. Daí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tZ^2_1}\\right]&=\\int_{-\\infty}^\\infty e^{tz^2}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac12z^2}\\,dz\n  = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{1-2t}}{\\sqrt{1-2t}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\n  = \\frac{1}{\\sqrt{1-2t}}, \\quad t<\\frac12.\n\\end{align*}\\]\n\nPelo Teorema 2.5, a FGM da distribuição de \\(U\\) é dada por\n\n\\[\\begin{align*}\n  M_U(t)&=\\left(\\frac{1}{\\sqrt{1-2t}}\\right)^k, \\quad t<\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeguindo o resultado do Exemplo 2.7, se \\(\\alpha=\\frac{k}{2}\\) e \\(\\beta=\\frac12\\), então \\[M_X(t)=\\left(1-2t\\right)^{-\\frac{k}{2}}, \\quad t<\\frac12.\\] Sendo a FGM de uma variável aleatória com distribuição \\(\\chi^2_{(k)}\\). Dessa forma, pelo Teorema 2.4, a distribuição da variável aleatória \\(U\\) é \\(\\chi^2_{(k)}\\).\n\n\n\n\n\nCorolário 2.2 Seja \\(X_1,X_2,\\ldots,X_k\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Então, \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\] é uma variável aleatória de uma população com distribição \\(\\chi^2_{(k)}\\).\n\n\n\nExemplo 2.13 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\). Mostre que \\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), daí\n\n\\[\\begin{align*}\n  (n-1)S^2_n&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\n  =\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1}+\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+\\sum_{i=1}^{n-1}(\\bar{X}_{n-1}-\\bar{X}_n)^2+2(\\bar{X}_{n-1}-\\bar{X}_n)\\underbrace{\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})}_{0}+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+(n-1)(\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=(n-2)S^2_{n-1}+(n-1)\\left(\\frac{n}{n(n-1)}\\sum_{i=1}^{n-1}X_i-\\frac{n-1}{n(n-1)}\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n^2(n-1)}\\left(nX_n-\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n-1}\\left(X_n-\\bar{X}_n\\right)^2+(X_n-\\bar{X}_n)^2\n    =(n-2)S^2_{n-1}+\\frac{n}{n-1}\\left(X_n-\\bar{X}_n\\right)^2\\\\\n  &=(n-2)S^2_{n-1}+\\frac{n-1}{n}\\left(X_n-\\bar{X}_{n-1}\\right)^2, \\quad n>1.\\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, aplicando indução matemática, temos que\n\nPara \\(n=2\\), temos que \\(S^2_2=\\frac{1}{2}\\left(X_2-X_1\\right)^2\\). Sendo \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\), então \\(X_2-X_1\\) segue uma distribuição normal com média \\(0\\) e variância \\(2\\sigma^2\\). Daí, \\(\\frac{\\left(X_2-X_1\\right)^2}{2\\sigma^2}=\\frac{S_2^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\);\nSuponha que, para \\(n=k\\), \\(\\frac{(k-1)S^2_k}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k-1)}\\);\nProvemos para \\(n=k+1\\),\n\n\n\\[\\begin{align*}\n  \\frac{kS^2_{k+1}}{\\sigma^2}&=\\frac{(k-1)S^2_{k}}{\\sigma^2}+\\frac{1}{\\sigma^2}\\frac{k}{k+1}\\left(X_{k+1}-\\bar{X}_{k}\\right)^2.\n\\end{align*}\\]\n\nObserve que, \\(X_{k+1}-\\bar{X}_{k}\\) segue uma distribuição normal com média \\(0\\) e variância \\(\\left(\\frac{k+1}{k}\\right)\\sigma^2\\). Daí, \\(\\frac{k}{k+1}\\frac{(X_{k+1}-\\bar{X}_{k})^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\). Pelo Teorema 2.6, \\(\\frac{kS^2_{k+1}}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k)}\\). \\(\\quad\\checkmark\\)\n\n\n\n\n2.2.1 Momentos fatoriais\n\nDefinição 2.6 (Função geradora de momentos fatoriais) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos fatoriais (FGMF) de \\(F_X\\), denotada por \\(\\mathcal{M}_X(t)\\), se existe, define-se como\n\\[\\mathcal{M}_X(t)=M_X(\\log t)=\\mathbb{E}\\left[t^{X}\\right],\\]\npara todo \\(t\\in\\mathbb{R}\\).\nSe \\(\\mathcal{M}_X(t)\\) existe em uma vizinhança de \\(t = 1\\), o \\(r\\)-ésimo momento fatorial é dado por \\[\\mathbb{E}\\left[(X)_r\\right]=\\mathbb{E}\\left[X(X-1)(X-2)\\cdots (X-r+1)\\right]=\\Biggr.\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}.\\]\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(\\mathcal{M}_Y(t)=t^a\\mathcal{M}_X(t^b)\\).\n\n\n\nExemplo 2.14 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{n}{x}p^x(1-p)^{n-x}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^n t^j \\binom{n}{j}p^j(1-p)^{n-j}=\\sum_{j=0}^n \\binom{n}{j}(tp)^j(1-p)^{n-j}=[(1-p)+tp]^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n[(1-p)+tp]^{n-1}p\\Biggr|_{t=1}=np;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=\\frac{\\partial^2}{\\partial t^2}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)p^2[(1-p)+tp]^{n-2}\\Biggr|_{t=1}=n(n-1)p^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)\\cdots(n-r+1)p^r=\\frac{n!}{(n-r)!}p^r=(n)_r\\,p^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.15 Seja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\frac{e^{-\\lambda}\\lambda^j}{\\lambda!}=e^{-\\lambda}\\sum_{j=0}^\\infty  \\frac{(t\\lambda)^j}{\\lambda!}=e^{-\\lambda}e^{t\\lambda}=e^{(t-1)\\lambda}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=e^{(t-1)\\lambda}\\,\\lambda\\Biggr|_{t=1}=\\lambda;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=e^{(t-1)\\lambda}\\,\\lambda^2\\Biggr|_{t=1}=\\lambda^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=e^{(t-1)\\lambda}\\,\\lambda^r\\Biggr|_{t=1}=\\lambda^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.16 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial negativa com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{x+n-1}{x}p^n(1-p)^{x}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGMF da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\binom{j+n-1}{j}p^n(1-p)^{j}=\\sum_{j=0}^\\infty \\binom{j+n-1}{j}p^n[t(1-p)]^{j}\\\\\n  &=p^n\\sum_{j=0}^\\infty \\binom{j+n-1}{j}[t(1-p)]^{j}=p^n[1-(1-p)t]^{-n}=\\left[\\frac{p}{1-(1-p)t}\\right]^n,  \\quad |t|<\\frac{1}{1-p}. \\quad\\checkmark\n\\end{align*}\\]\nA última soma é resultado de aplicar os resultados da Seção A.2 do Apêndice A.\n\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nSe \\(X\\) é uma variável aleatória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\), então \\(\\mathcal{M}_X(t)\\) é chamada de função geradora de probabilidades (FGP). Nesse caso, \\(\\mathcal{M}_X(t)\\) existe para todo \\(|t|\\le1\\).\nA \\(r\\)-ésima probabilidade de massa de \\(X\\) calcula-se como \\[\\mathbb{P}\\left[X=r\\right]=\\frac{1}{k!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}.\\]\n\n\n\nExemplo 2.17 Considere o cenário do Exemplo 2.14. Utilize a FGP da distribuição de \\(X\\) para calcular a \\(r\\)-ésima probabilidade de massa.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo Exemplo 2.14, sabemos que a FGP é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\). Daí,\n\n\\[\\begin{align*}\n  \\mathbb{P}\\left[X=0\\right]&=[(1-p)+tp]^n\\Biggr|_{t=0}=(1-p)^n; \\\\ \\\\\n  \\mathbb{P}\\left[X=1\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=0}=np(1-p)^{n-1}; \\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{P}\\left[X=r\\right]&=\\frac{1}{r!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}=\\frac{n(n-1)\\cdots(n-r+1)}{r!}p^r(1-p)^{n-r}\\\\\n  &=\\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}=\\binom{n}{r}p^r(1-p)^{n-r}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\n\n2.2.2 Cumulantes\n\nDefinição 2.7 (Função geradora de cumulantes) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de cumulantes (FGC) de \\(F_X\\), define-se como\n\\[K_X(t)=\\log M_X(t)=\\sum_{j=1}^\\infty\\kappa_n\\frac{t^j}{j!},\\]\nonde \\(M_X(t)\\) representa a FGM de \\(F_X\\), \\(|t|<h\\), \\(h>0\\). As constantes \\(\\kappa_1,\\kappa_2,\\kappa_3,\\ldots\\) são chamados de cumulantes da distribuição \\(F_X\\).\n\n\n\n\n\n\n\nDica\n\n\n\nOs cumulantes são funções dos momentos e podem ser obtidos diferenciando a função \\(K_X(t)\\)\n\\[\\kappa_j=K'_X(0)=\\frac{\\partial^j}{\\partial t^j}K_X(t)\\Biggr|_{t=0},\\] para \\(j=1,2,3,\\ldots\\).\n\n\n\nExemplo 2.18 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes. Mostre que \\[K_{X+Y}(t)=K_X(t)+K_Y(t).\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando a Definição 2.7, temos que\n\n\\[\\begin{align*}\n  K_{X+Y}(t)&=\\log\\mathbb{E}[e^{t(X+Y)}]=\\log\\mathbb{E}[e^{tX}e^{tY}]\n  =\\log\\left(\\mathbb{E}[e^{tX}]\\mathbb{E}[e^{tY}]\\right)\\\\\n  &=\\log\\mathbb{E}[e^{tX}]+\\log\\mathbb{E}[e^{tY}]=K_X(t)+K_Y(t). \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.19 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\). Utilize a FGC para calcular o \\(r\\)-ésimo cumulante da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.6, sabemos que \\(\\mathcal{M}_X(t)=M_X(\\log t)\\). Pelo Exemplo 2.14, sabemos que a FGMF de uma variável aleatória com distribuição binomial é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\), com \\(t\\in\\mathbb{R}\\). Assim, a FGM de uma variável aleatória com distribuição Bernoulli é dada por \\(M_X(t)=(1-p)+e^tp\\), com \\(t\\in\\mathbb{R}\\). Daí, \\(K_{X}(t)=\\log M_X(t)=\\log[(1-p)+e^tp]\\).\nDessa forma,\n\n\\[\\begin{align*}\n  K'_{X}(0)&=\\kappa_1=\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}=p\\\\\n  K''_{X}(0)&=\\kappa_2=e^tp\\frac{(1-p)+e^tp-e^tp}{((1-p)+e^tp)^2}\\Biggr|_{t=0}=p(1-p)\\\\\n  K'''_{X}(0)&=\\kappa_3=\\frac{[(1-p)+e^tp]^2p(1-p)e^t-2p(1-p)e^t(e^tp+p(1-p)e^t)}{((1-p)+e^tp)^4}\\Biggr|_{t=0}=p(1-p)(1-2p)\\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  K^{(r)}_X(0)&=\\kappa_r=\\frac{\\partial^r}{\\partial t^r}K_X(t)\\Biggr|_{t=0}.\n\\end{align*}\\]\n\nPara calcular a \\(r\\)-ésima derivada de \\(K_X(t)\\), note que\n\n\\[\\begin{align*}\n  \\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial}{\\partial p}\\left[\\frac{\\partial^r}{\\partial t^r}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial p}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outra parte,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}&=\\frac{\\partial^{r+1}}{\\partial t^{n+1}}K_X(t)\\Biggr|_{t=0}\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial t}K_X(t)\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}-p(1-p)\\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}-p(1-p)\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp^2+p(1-p)}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]=\\frac{\\partial^r}{\\partial t^r}p=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, para \\(r>1\\) \\[\\kappa_{r+1}=p(1-p)\\frac{\\partial}{\\partial p}\\kappa_r. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nDa Definição 2.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\) existe para todo \\(|t|<h\\), para algum \\(h>0\\). Se \\(\\mathbb{E}X^r\\) existe para todo \\(r=1,2,\\ldots\\), então\n\\[M_X(t)=\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r, \\quad |t|<h.\\] Dessa forma,\n\n\\[\\begin{align*}\n  K_X(t)&=\\log M_X(t)=\\log\\left[\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\n  =\\log\\left[1+\\sum_{r=1}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\\\\\n  &=\\mathbb{E}[X]\\, t+\\frac{1}{2!}\\left[\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right]t^2\n  +\\frac{1}{3!}\\left[\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X\\right]t^3+\\cdots.\n\\end{align*}\\]\n\nDaí, \\(\\kappa_1=\\mathbb{E}X\\), \\(\\kappa_2=\\mathbb{E}X^2-(\\mathbb{E}X)^2\\), \\(\\kappa_3=\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X=\\mathbb{E}\\left[(X-\\mathbb{E}X)^3\\right]\\), \\(\\kappa_4=\\mathbb{E}\\left[(X-\\mathbb{E}X)^4\\right]-3\\kappa_2^2\\), etc."
  },
  {
    "objectID": "matematica.html#sec-combinatorias",
    "href": "matematica.html#sec-combinatorias",
    "title": "Appendix A — Resultados matemáticos",
    "section": "A.2 Fatoriais e Combinatórias",
    "text": "A.2 Fatoriais e Combinatórias\n\nFunção gamma: \\(\\Gamma(x)=\\int_0^\\infty t^{x-1}e^{-t}\\,dt\\), para \\(x>0\\). Dessa forma, \\(\\Gamma(x+1)=x\\Gamma(x)\\);\n\nSe \\(n\\) é inteiro, então \\(\\Gamma(n+1)=n!\\), onde \\(n!=1\\cdot2\\cdot3\\cdots (n-1)\\cdot n\\);\nSe \\(n\\) é inteiro, \\(\\Gamma\\left(n+\\frac12\\right)=\\frac{1\\cdot3\\cdot5\\cdots(2n-1)}{2^n}\\sqrt\\pi\\). Em particular, \\(\\Gamma\\left(\\frac32\\right)=\\frac12\\Gamma\\left(\\frac12\\right)=\\frac12\\sqrt\\pi\\) ou \\(\\Gamma\\left(\\frac12\\right)=\\sqrt\\pi\\);\nFórmula de Stirling: \\(n!\\approx\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n\\)\n\nFunção beta: \\(B(a,b)=\\int_0^1x^{a-1}(1-x)^{b-1}\\,dx\\), para \\(a>0\\) e \\(b>0\\). De outra forma, \\(B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\);\n\\(\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\);\n\n\\(\\binom{n}{0}=\\binom{n}{n}=1\\);\nTeorema das combinações complementares: \\(\\binom{n}{k}=\\binom{n}{n-k}\\) ;\nRelação de Stifel: \\(\\binom{n+1}{k}=\\binom{n}{k}+\\binom{n}{k-1}\\), para \\(n=1,2,\\ldots\\) e \\(k=0,\\pm1,\\pm2,\\ldots\\);\n\\(\\binom{-n}{k}=\\frac{(-n)(-n-1)\\cdots(-n-k+1)}{k!}=(-1)^k\\binom{n+k-1}{k}\\);\n\\(\\binom{n}{k}\\binom{n-k}{m-k}=\\binom{n}{m}\\binom{m}{k}\\);\n\nSérie binomial negativa: Sendo \\(n\\) inteiro negativo, para todo \\(|x|<a\\), temos que \\[(x+a)^{-n}=\\sum_{k=0}^\\infty\\binom{-n}{k}x^ka^{-n-k}=\\sum_{k=0}^\\infty(-1)^k\\binom{n+k-1}{k}x^ka^{-n-k};\\]"
  },
  {
    "objectID": "matematica.html#constantes",
    "href": "matematica.html#constantes",
    "title": "Appendix A — Resultados matemáticos",
    "section": "A.3 Constantes",
    "text": "A.3 Constantes\n\n\\(\\pi=4\\sum_{j=0}^\\infty\\frac{(-1)^j}{2j+1}\\);\n\\(e=\\sum_{j=0}^\\infty\\frac{1}{j!}\\);\n\\(\\log2=\\sum_{j=0}^\\infty\\frac{(-1)^{j-1}}{j}\\)\n\n\n\n\n\nMood, A., F. Graybill, e D. Boes. 1974. Introduction to the theory of statistics. 3rd ed. McGraw-Hill Higher Education."
  },
  {
    "objectID": "chap02.html#função-característica",
    "href": "chap02.html#função-característica",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.3 Função característica",
    "text": "2.3 Função característica"
  },
  {
    "objectID": "chap02.html#função-geradora-de-moemntos-conjunta",
    "href": "chap02.html#função-geradora-de-moemntos-conjunta",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.3 Função geradora de moemntos conjunta",
    "text": "2.3 Função geradora de moemntos conjunta"
  },
  {
    "objectID": "chap02.html#função-geradora-de-momentos-conjunta",
    "href": "chap02.html#função-geradora-de-momentos-conjunta",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.3 Função geradora de momentos conjunta",
    "text": "2.3 Função geradora de momentos conjunta\n\nDefinição 2.8 Seja \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)^\\intercal\\) uma vetor aleatório de tamanho \\(n\\times1\\). A função geradora de momentos conjunta (FGMC) define-se, como\n\\[M_{\\mathbf{X}}(\\mathbf{t})=\\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]=\\mathbb{E}\\left[e^{\\sum_{i=1}^nt_iX_i}\\right],\\] se existe para todos os vetores reais \\(\\mathbf{t}=(t_1,t_2,\\ldots,t_n)\\) que pertencem a um retângulo rechado \\(H\\), tal que\n\\[H=[-h_1,h_1]\\times[-h_2,h_2]\\times\\cdots\\times[-h_n,h_n]\\subset\\mathbb{R}^n,\\] com \\(h_i>0\\) para todo \\(i=1,2,3,\\ldots, n\\).\n\n\nExemplo 2.20 Seja \\(\\mathbf{X}=(X_1,X_2)\\) um vetor aleatório de uma população com função de densidade conjunta dada por\n\\[f(x_1,x_2)=e^{-(x_1+x_2)}, \\quad x_1>0, x_2>0.\\]\nCalcule a FGMC da distribuição de \\(\\mathbf{X}=(X_1,X_2)\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.8, sabemos que\n\n\\[\\begin{align*}\n  M_{\\mathbf{X}}(\\mathbf{t}) &= \\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]\n  = \\mathbb{E}[e^{t_1X_1+t_2X_2}] = \\int_{0}^\\infty \\int_{0}^\\infty  e^{t_1x_1+t_2x_2}e^{-(x_1+x_2)}\\,dx_1dx_2 \\\\\n  &= \\int_{0}^\\infty \\int_{0}^\\infty e^{(t_1-1)x_1}e^{(t_2-1)x_2}\\,dx_1dx_2\n  = \\int_{0}^\\infty e^{(t_1-1)x_1}\\,dx_1 \\int_{0}^\\infty e^{(t_2-1)x_2}\\,dx_2\\\\\n  &= \\frac{1}{(1-t_1)(1-t_2)}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.21 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Encontre as distribuições de \\(2X_1X_2\\) e \\(X_2^2-X_1^2\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(X_1\\) e \\(X_2\\) são variáveis aleatórias independentes e identicamente distribuidas com distribuição \\(N(0,1)\\). Dessa forma, \\(2X_1X_2\\) tem distribuição \\(2\\chi_{(1)}^2\\), ou seja \\(Gama\\left(\\frac12,\\frac14\\right)\\).\nSeja \\(U=2X_1X_2\\), então\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tU}\\right]&=\\mathbb{E}\\left[e^{2tX_1X_2}\\right]=\n      \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty e^{2tx_1x_2}e^{-\\frac12(x_1^2+x_2^2)}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-\\frac12x_2^2}\\int_{-\\infty}^\\infty e^{-\\frac12x_1^2}e^{2tx_1x_2}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12x_2^2\\right\\}\\exp\\left\\{2t^2x_2^2\\right\\}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(x_1-2tx_2\\right)^2\\right\\}\\,dx_1dx_2\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(1-4t^2\\right)x_ 2^2\\right\\}\\,dx_2\\\\\n      &=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2<\\frac14 \\text{ ou } |t|<\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeja \\(t^*=t^2\\), então \\(\\mathbb{E}\\left[e^{t^*U}\\right]=\\left(1-4t^*\\right)^{-\\frac12}\\), com \\(t^*<\\frac{1}{4}\\), i.e., a função geradora de momentos de uma \\(Gama(\\frac12,\\frac14)\\).\nConsidere \\(V=X_2^2-X_1^2\\), então\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[e^{tV}\\right]&=\\mathbb{E}\\left[e^{t\\left(X_2^2-X_1^2\\right)}\\right]=\\mathbb{E}\\left[e^{tX_2^2}\\right]\\mathbb{E}\\left[e^{- tX_1^2}\\right]=\\left(1-2t\\right)^{-\\frac12}\\left(1+2t\\right)^{-\\frac12}\\\\\n      &=\\left[\\left(1-2t\\right)\\left(1+2t\\right)\\right]^{-\\frac12}=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2<\\frac14 \\text{ ou } |t|<\\frac12.\n\\end{align*}\\]\n\nIsso significa que \\(U\\) e \\(V\\) seguem a mesma distribuição \\(2\\chi_{(1)}^2\\) ou \\(gama(\\frac12,\\frac14)\\quad\\checkmark\\)."
  }
]